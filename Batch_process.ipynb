{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook n√†y ch·ª©a c√°c ƒëo·∫°n code s·ª≠ d·ª•ng ƒë·ªÉ x·ª≠ l√Ω h√†ng lo·∫°t tr√™n d·ªØ li·ªáu. N·∫øu mu·ªën hi·ªÉu th√™m chi ti·∫øt v·ªÅ ƒë·ªì √°n, vui l√≤ng chuy·ªÉn ƒë·∫øn Final.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ƒê·ªì √°n cu·ªëi k·ª≥ - Ph√¢n t√≠ch ch·ªß ƒë·ªÅ vƒÉn b·∫£n\n",
    "\n",
    "Nh√≥m: 12\n",
    "\n",
    "Th√†nh vi√™n nh√≥m:\n",
    "- V≈© ƒêƒÉng Ho√†ng Long - MSSV: 18120203\n",
    "- Nguy·ªÖn Hu·ª≥nh ƒê·∫°i L·ª£i - MSSV: 18120198"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Gi·ªõi thi·ªáu ƒë·ªì √°n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ch·ªß ƒë·ªÅ: Nh·∫≠n di·ªán ch·ªß ƒë·ªÅ c·ªßa m·ªôt ƒëo·∫°n vƒÉn b·∫£n b·∫•t k·ª≥.\n",
    "\n",
    "Input: m·ªôt ƒëo·∫°n vƒÉn b·∫£n b·∫•t k·ª≥.\n",
    "\n",
    "Output: m·ªôt trong 18 ph√¢n l·ªõp sau:\n",
    "\n",
    "| | | |\n",
    "| :- | :- | :- |\n",
    "| 1. th·ªùi s·ª± qu·ªëc t·∫ø | 2. th·ªùi s·ª± trong n∆∞·ªõc | 3. du l·ªãch |\n",
    "| 4. kinh doanh | 5. gi·∫£i tr√≠ | 6. c√¥ng ngh·ªá |\n",
    "| 7. nh√† ƒë·∫•t | 8. s·ª©c kh·ªèe | 9. gi√°o d·ª•c |\n",
    "| 10. khoa h·ªçc | 11. th·ªÉ thao | 12. vƒÉn h√≥a |\n",
    "| 13. ph√°p lu·∫≠t | 14. y√™u | 15. xe |\n",
    "| 16. th·ªùi trang | 17. nh·ªãp s·ªëng tr·∫ª | 18. ƒÉn g√¨ |\n",
    "\n",
    "Ngu·ªìn d·ªØ li·ªáu: t·∫•t c·∫£ b√†i b√°o thu th·∫≠p t·ª´ trang b√°o ƒëi·ªán t·ª≠ Tu·ªïi tr·∫ª Online (https://tuoitre.vn/).\n",
    "\n",
    "M·ª•c ƒë√≠ch:\n",
    "- Kh√°ch quan: ph·ª•c v·ª• vi·ªác nh·∫≠n di·ªán ch·ªß ƒë·ªÅ m·ªôt c√°ch t·ª± ƒë·ªông.\n",
    "- Ch·ªß quan: l·ªçc c√°c b√†i vi·∫øt tr√™n m·∫°ng x√£ h·ªôi theo ch·ªß ƒë·ªÅ m√† em quan t√¢m ƒë·ªÉ tr√°nh l√£ng ph√≠ th·ªùi gian l∆∞·ªõt facebook ch·ªâ ƒë·ªÉ t√¨m ch·ªß ƒë·ªÅ m√† em quan t√¢m ü•¥."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Ph·∫ßn ƒë·ªì √°n (x·ª≠ l√Ω h√†ng lo·∫°t to√†n d·ªØ li·ªáu)\n",
    "\n",
    "## C√†i ƒë·∫∑t th∆∞ vi·ªán\n",
    "\n",
    "(c√≥ th·ªÉ b·ªè qua cell ·ªü ƒë√¢y n·∫øu c√†i r·ªìi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# C√†i ƒë·∫∑t th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
    "!pip3 install pandas;\n",
    "!pip3 install tqdm;\n",
    "!pip3 install bs4;\n",
    "!pip3 install regex;\n",
    "!pip3 install numpy;\n",
    "!pip3 install pyvi;\n",
    "!pip3 install gensim;\n",
    "!pip3 install matplotlib;\n",
    "!pip3 install seaborn;\n",
    "!pip3 install -U scikit-learn;\n",
    "!pip3 install lime;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import th∆∞ vi·ªán"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tqdm\\std.py:668: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "plt.rcParams['figure.dpi'] = 125\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import time # D√πng ƒë·ªÉ sleep ch∆∞∆°ng tr√¨nh\n",
    "from tqdm.notebook import tqdm # Hi·ªán thanh progress cho ƒë·∫πp :D\n",
    "tqdm.pandas()\n",
    "\n",
    "# Th∆∞ vi·ªán ƒë·ªÉ request v√† parse HTML\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# C√°c th∆∞ vi·ªán li√™n quan t·ªõi ng√¥n ng·ªØ v√† NLP\n",
    "from pyvi import ViTokenizer # Th∆∞ vi·ªán NLP ti·∫øng Vi·ªát\n",
    "import gensim\n",
    "import unicodedata # Th∆∞ vi·ªán unicode\n",
    "\n",
    "# Tr·ª±c quan h√≥a vƒÉn b·∫£n\n",
    "from lime import lime_text\n",
    "\n",
    "# D√πng ƒë·ªÉ l∆∞u l·∫°i model\n",
    "import pickle\n",
    "\n",
    "# Th∆∞ vi·ªán li√™n quan c·ªßa Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn import feature_selection\n",
    "\n",
    "# T·∫°o pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import make_column_transformer, make_column_selector\n",
    "\n",
    "# C√°c m√¥ h√¨nh h·ªçc\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import BaggingClassifier # Ph∆∞∆°ng ph√°p bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Thu th·∫≠p d·ªØ li·ªáu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thi·∫øt l·∫≠p ƒë∆∞·ªùng d·∫´n cho ph·∫ßn 1\n",
    "dir_1 = \"src/scraped_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "---\n",
    "## 1. Thu th·∫≠p d·ªØ li·ªáu\n",
    "\n",
    "# Thi·∫øt l·∫≠p ƒë∆∞·ªùng d·∫´n cho ph·∫ßn 1\n",
    "dir_1 = \"src/scraped_data/\"def single_request_scraping(index = 1, limit_retry = 100, sleep_time = 0.05):\n",
    "    '''\n",
    "    Thu th·∫≠p c√°c trang tin trong m·ª•c tin m·ªõi nh·∫•t c·ªßa b√°o Tu·ªïi Tr·∫ª.\n",
    "    M·ªói l·∫ßn thu th·∫≠p 20 tin. S·ª≠ d·ª•ng s·ªë index ƒë·ªÉ thu th·∫≠p c√°c trang li√™n ti·∫øp.\n",
    "    V·ªõi index = 1 t·ª©c l·∫•y 20 trang tin m·ªõi nh·∫•t.\n",
    "\n",
    "    L∆∞u √Ω:\n",
    "        Khi l·∫•y li√™n t·ª•c c·∫ßn ch·ªçn th·ªùi ƒëi·ªÉm\n",
    "        v√¨ c√≥ kh·∫£ nƒÉng b√°o c·∫≠p nh·∫≠t tin t·ª©c m·ªõi s·∫Ω l√†m \n",
    "        tin ·ªü trang tr∆∞·ªõc b·ªã ƒë·∫©y xu·ªëng trang sau.\n",
    "\n",
    "    Param:\n",
    "        index: s·ªë ch·ªâ trang c·∫ßn request.\n",
    "    '''\n",
    "    allow_status = [200, 301]\n",
    "    s = requests.Session()\n",
    "    s.headers['User-Agent'] = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/34.0.1847.131 Safari/537.36'\n",
    "    url = f\"https://tuoitre.vn/timeline/0/trang-{index}.htm\"\n",
    "\n",
    "    # L·∫•y danh s√°ch news_items\n",
    "    try:\n",
    "        response = s.get(url)\n",
    "        try_left = limit_retry\n",
    "        while (response.status_code not in allow_status and try_left > 0):\n",
    "            print(f\"Loi {response.status_code} tai trang {index}\")\n",
    "            response = s.get(url)\n",
    "            try_left -= 1\n",
    "        html_text = response.text\n",
    "    except:\n",
    "        print(f\"Loi request tai trang {index}\")\n",
    "        return None\n",
    "    \n",
    "    html_tree = BeautifulSoup(html_text, 'html.parser')\n",
    "    news_items = html_tree.findAll('li', class_='news-item')\n",
    "    \n",
    "    # Ki·ªÉm tra l·ªói n·∫øu kh√¥ng l·∫•y ƒë∆∞·ª£c b·∫•t c·ª© item n√†o\n",
    "    if (len(news_items) == 0):\n",
    "        print(f'Trang {index} khong lay duoc item.')\n",
    "        print(html_text)\n",
    "        return None\n",
    "    elif (len(news_items) != 20):\n",
    "        print(f\"Warning: Trang {index} chi lay duoc {len(news_items)} items.\")\n",
    "    \n",
    "    # L·∫•y ra link, title v√† category t·ª´ news_items\n",
    "    raw_data = pd.DataFrame(columns=[\"links\",\"title\",\"description\",\"content\",\"class\"])\n",
    "    for item in news_items:\n",
    "        try:\n",
    "            title = item.find('h3', class_='title-news').text.replace('\\n','')\n",
    "        except:\n",
    "            title = \"\"\n",
    "        link = \"https://tuoitre.vn\" + item.find('a').attrs[\"href\"]\n",
    "        try:\n",
    "            category = item.find('a', class_='category-name').text\n",
    "        except:\n",
    "            category = \"\"\n",
    "        raw_data = raw_data.append({\"links\":link, \"title\":title, \"class\":category}, ignore_index=True)\n",
    "    \n",
    "    # Ti·∫øn h√†nh l·∫•y n·ªôi dung t·ª´ng link\n",
    "    for _, row in raw_data.iterrows():\n",
    "        try:\n",
    "            response = s.get(row[\"links\"])\n",
    "            try_left = limit_retry\n",
    "            while (response.status_code not in allow_status and try_left > 0):\n",
    "                print(f\"Loi {response.status_code} tai link {row['links']}\")\n",
    "                response = s.get(row[\"links\"])\n",
    "                try_left -= 1\n",
    "            news_page = response.content\n",
    "        except:\n",
    "            print(f\"Loi request tai link {row['links']}\")\n",
    "            row[\"description\"] = \"\"\n",
    "            row[\"content\"] = \"\"\n",
    "            continue\n",
    "            \n",
    "        news_tree = BeautifulSoup(news_page, \"html.parser\")\n",
    "        # L·∫•y m√¥ t·∫£\n",
    "        try:\n",
    "            row[\"description\"] = news_tree.find(\"h2\", class_=\"sapo\").text\n",
    "        except:\n",
    "            row[\"description\"] = ''\n",
    "        # L·∫•y n·ªôi dung\n",
    "        try:\n",
    "            body = news_tree.find(\"div\", id=\"main-detail-body\")\n",
    "            content = body.findChildren(\"p\", recursive=False)\n",
    "            row[\"content\"] = \"\"\n",
    "            for x in content:\n",
    "                row[\"content\"] += x.text\n",
    "        except:\n",
    "            row[\"content\"] = ''\n",
    "        time.sleep(sleep_time)\n",
    "    \n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Batch scraping\n",
    "def batch_scraping(num = 200, output_dir = \"\"):\n",
    "    '''\n",
    "    Th·ª±c hi·ªán thu th·∫≠p to√†n b·ªô d·ªØ li·ªáu tr√™n trang tuoitre.vn\n",
    "    \n",
    "    Param:\n",
    "        num: s·ªë l∆∞·ª£ng trang request cho 1 batch (1 trang = 20 b√†i b√°o). M·∫∑c ƒë·ªãnh 4000 b√†i b√°o.\n",
    "    '''\n",
    "    iter_num = 0  # S·ªë batch b·∫Øt ƒë·∫ßu\n",
    "    continue_flag = True # C·ªù hi·ªáu k·∫øt th√∫c v√≤ng l·∫∑p khi x·∫£y ra l·ªói\n",
    "\n",
    "    while (continue_flag):\n",
    "        \n",
    "        '''\n",
    "        Kh·ªüi t·∫°o dataframe r·ªóng. \n",
    "        Sau ƒë√≥ l·∫•y ƒë·ªß 1 s·ªë trang cho 1 batch.\n",
    "        R·ªìi export file csv.\n",
    "        '''\n",
    "        batch_df = pd.DataFrame(columns=[\"links\",\"title\",\"description\",\"content\",\"class\"])\n",
    "        for index in range(iter_num*num+1,(iter_num+1)*num+1):\n",
    "            data = single_request_scraping(index)\n",
    "            if (data is None):\n",
    "                continue_flag = False\n",
    "                break\n",
    "            print(f\"Page {index} complete!\")\n",
    "            batch_df = batch_df.append(data)\n",
    "        batch_df.to_csv(output_dir + f'crawling_{iter_num}.csv',index=False,encoding=\"utf-8\")    \n",
    "        iter_num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_scraping(output_dir = dir_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Ti·ªÅn x·ª≠ l√Ω b∆∞·ªõc ƒë·∫ßu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_prepreprocess():\n",
    "    total_files = 218\n",
    "    text_attrs = [\"title\",\"description\",\"content\",\"class\"]\n",
    "    prev_df = None\n",
    "    \n",
    "    for index in tqdm(range(total_files)):\n",
    "        df=pd.read_csv(f'src/scraped_data/crawling_{index}.csv')\n",
    "        # X√≥a d√≤ng thi·∫øu\n",
    "        df.dropna(axis=0, how=\"any\", inplace=True)\n",
    "        # X√≥a d√≤ng tr√πng\n",
    "        df.drop_duplicates(inplace=True)\n",
    "        # X√≥a d√≤ng tr√πng v·ªõi file tr∆∞·ªõc\n",
    "        if (prev_df is not None):\n",
    "            df = df.merge(prev_df, how='outer', indicator=True).loc[lambda x : x['_merge'] == 'left_only']\n",
    "            df.drop(['_merge'],axis=1,inplace=True)\n",
    "        # Chu·∫©n h√≥a unicode v√† d·∫•u\n",
    "        for attr in text_attrs:\n",
    "            df[attr]=df[attr].apply(covert_unicode)\n",
    "            df[attr]=df[attr].str.lower()\n",
    "            df[attr]=df[attr].apply(chuan_hoa_dau_cau_tieng_viet)\n",
    "        df.to_csv(f'src/scraped_data/crawling_{index}.csv', index=False)\n",
    "        prev_df = df\n",
    "        \n",
    "    print(\"done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_prepreprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Ti·ªÅn x·ª≠ l√Ω sau c√πng to√†n d·ªØ li·ªáu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
