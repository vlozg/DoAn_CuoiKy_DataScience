{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Đồ án cuối kỳ - Phân tích chủ đề văn bản\n",
    "(Cập nhật 07/01/2021)\n",
    "\n",
    "Nhóm: 12\n",
    "\n",
    "Thành viên nhóm:\n",
    "- Vũ Đăng Hoàng Long - MSSV: 18120203\n",
    "- Nguyễn Huỳnh Đại Lợi - MSSV: 18120198"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Import thư viện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time # Dùng để sleep chương trình\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Thu thập dữ liệu"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dữ liệu trong đồ án này được thu thập toàn bộ từ trang báo mạng Tuổi Trẻ Online (https://tuoitre.vn/).\n",
    "\n",
    "Khi mới vào trang chủ, có thể thấy trang báo này phân bổ thứ tự và cấu trúc thông tin không đều, rất khó để có thể thu thập được. Tuy nhiên, may mắn là trang báo này có mục tin mới nhất chứa toàn bộ tin từ các chuyên mục và có tổ chức các mục có cấu trúc rõ ràng (Hình 1).\n",
    "\n",
    "![Hình 1](resources/tin-moi-nhat.png \"Hình 1\")\n",
    "\n",
    "Khi người dùng kéo xuống cuối trang, sẽ có nút \"Xem thêm\" (Hình 2). Người dùng nhấn vào đây và tin tức cũ hơn sẽ hiện ngay bên dưới để người dùng kéo xuống tiếp.\n",
    "\n",
    "![Hình 2](resources/xem-them.png \"Hình 2\")\n",
    "\n",
    "Thoáng nhìn qua, quy trình lướt tin tức sẽ bao gồm: Vào mục tin mới -> Kéo xuống cuối trang -> Bấm \"Xem thêm\" -> Kéo xuống cuối trang và lặp lại. Như vậy với cách làm đơn giản nhất là mô phỏng click chuột của người dùng để bấm nút xem thêm càng nhiều càng tốt, sau đó lấy file HTML của trang về và thực hiện parse (Đây chính là cách làm trong phiên bản đầu của quy trình này của nhóm). Tuy nhiên cách này rất thiếu hiệu quả bởi lẽ máy sẽ cần phải mở trình duyệt lên và mô phỏng thao tác của người dùng -> rất chậm do thời gian tải trang và tốn tài nguyên xử lý của máy tính do phải render trang web. Trong phiên bản đầu nhóm đã mất khoảng vài phút để có thể lấy được 480 tin.\n",
    "\n",
    "Nhận thấy rằng trang tin này có một đặc điểm là có thể lăn chuột vô tận, nhóm tìm hiểu và phát hiện rằng có thể cải thiện hiệu suất của quy trình này lên rất nhiều lần! Cụ thể, khi người dùng bấm vào nút \"Xem thêm\", trang web sẽ gửi yêu cầu lên server và nhận về một file HTML có cấu trúc đơn giản chứa thông tin các bài báo (Hình 3). Sau đó trình duyệt sẽ thực hiện thay đổi DOM của trang web để thêm các tin này vào bên dưới, tạo hiệu ứng lăn chuột vô tận. Do vậy, thay vì phải đợi trình duyệt render trang web, nhóm có thể request thẳng đường link giống cách trang web request lên server. Việc làm này sẽ rút ngắn rất nhiều thời gian của quá trình thu thập dữ liệu!\n",
    "\n",
    "![Hình 3](resources/xem-them-network.png \"Hình 3\")\n",
    "\n",
    "Khi sử dụng công cụ theo dõi network của trình duyệt, nhóm phát hiện ra rằng đường link request tin mới của trang web khi bấm nút xem thêm lần đầu là \"https://tuoitre.vn/timeline/0/trang-1.htm\". Rất có thể khi thay thế trang-1 thành trang-2, trang-3,... chúng ta sẽ thu được các kết quả là tin mới của các lần bấm thứ 2, thứ 3,... vào nút \"Xem thêm\" (điều này đúng, nhóm đã kiểm chứng).\n",
    "\n",
    "Đoạn code bên dưới sẽ thực hiện request vào đường link trên và thực hiện parse với BeautifulSoup như HTML bình thường để lấy đường link và chuyên mục của các tin tức. Sau đó sẽ thực hiện request trang tin chính từ đường link thu thập được và lấy các thông tin tiêu đề, mô tả và nội dung.\n",
    "\n",
    "Kết quả sẽ được in bằng hàm tail() giúp có thể nắm được số lượng data lấy được thông qua index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>links</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>content</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>https://tuoitre.vn/1-ca-covid-19-moi-phat-hien...</td>\n",
       "      <td>1 ca COVID-19 mới, phát hiện sau 3 lần xét ngh...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sức khỏe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>https://tuoitre.vn/lang-kinh-24g-canh-giac-voi...</td>\n",
       "      <td>Lăng kính 24g: Cảnh giác với tội phạm trộm cắp...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cần biết</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>https://tuoitre.vn/game-show-cuoi-cung-chi-tai...</td>\n",
       "      <td>Gameshow cuối cùng của Chí Tài - Cơ hội đổi đờ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Giải trí</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>https://tuoitre.vn/giua-kho-khan-thoi-covid-19...</td>\n",
       "      <td>Giữa khó khăn 'thời COVID-19', một doanh nghiệ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thời sự</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>https://tuoitre.vn/triet-pha-duong-day-mang-th...</td>\n",
       "      <td>Triệt phá đường dây mang thai hộ, mỗi lần 'đẻ ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pháp luật</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                links  \\\n",
       "15  https://tuoitre.vn/1-ca-covid-19-moi-phat-hien...   \n",
       "16  https://tuoitre.vn/lang-kinh-24g-canh-giac-voi...   \n",
       "17  https://tuoitre.vn/game-show-cuoi-cung-chi-tai...   \n",
       "18  https://tuoitre.vn/giua-kho-khan-thoi-covid-19...   \n",
       "19  https://tuoitre.vn/triet-pha-duong-day-mang-th...   \n",
       "\n",
       "                                                title description content  \\\n",
       "15  1 ca COVID-19 mới, phát hiện sau 3 lần xét ngh...         NaN     NaN   \n",
       "16  Lăng kính 24g: Cảnh giác với tội phạm trộm cắp...         NaN     NaN   \n",
       "17  Gameshow cuối cùng của Chí Tài - Cơ hội đổi đờ...         NaN     NaN   \n",
       "18  Giữa khó khăn 'thời COVID-19', một doanh nghiệ...         NaN     NaN   \n",
       "19  Triệt phá đường dây mang thai hộ, mỗi lần 'đẻ ...         NaN     NaN   \n",
       "\n",
       "        class  \n",
       "15   Sức khỏe  \n",
       "16   Cần biết  \n",
       "17   Giải trí  \n",
       "18    Thời sự  \n",
       "19  Pháp luật  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Thử nghiệm url đầu tiên\n",
    "url = \"https://tuoitre.vn/timeline/0/trang-1.htm\"\n",
    "\n",
    "# Lấy danh sách news_items\n",
    "html_text = requests.get(url).text\n",
    "html_tree = BeautifulSoup(html_text, 'html.parser')\n",
    "news_items = html_tree.findAll('li', class_='news-item')\n",
    "\n",
    "# Lấy ra link, title và category từ news_items\n",
    "raw_data = pd.DataFrame(columns=[\"links\",\"title\",\"description\",\"content\",\"class\"])\n",
    "for item in news_items:\n",
    "    title = item.find('h3', class_='title-news').text.replace('\\n','')\n",
    "    link = \"https://tuoitre.vn\" + item.find('a').attrs[\"href\"]\n",
    "    category = item.find('a', class_='category-name').text\n",
    "    raw_data = raw_data.append({\"links\":link, \"title\":title, \"class\":category}, ignore_index=True)\n",
    "\n",
    "raw_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.83 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>links</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>content</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>https://tuoitre.vn/1-ca-covid-19-moi-phat-hien...</td>\n",
       "      <td>1 ca COVID-19 mới, phát hiện sau 3 lần xét ngh...</td>\n",
       "      <td>TTO - Sau nhiều ngày số mắc mới khá cao và dồn...</td>\n",
       "      <td>Theo Bộ Y tế, 1 ca mắc mới ngày 6-1 là ca nhập...</td>\n",
       "      <td>Sức khỏe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>https://tuoitre.vn/lang-kinh-24g-canh-giac-voi...</td>\n",
       "      <td>Lăng kính 24g: Cảnh giác với tội phạm trộm cắp...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Cần biết</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>https://tuoitre.vn/game-show-cuoi-cung-chi-tai...</td>\n",
       "      <td>Gameshow cuối cùng của Chí Tài - Cơ hội đổi đờ...</td>\n",
       "      <td>TTO - Tối 5-1, tập đầu tiên của chương trình t...</td>\n",
       "      <td>Cơ hội đổi đời là chương trình truyền hình thự...</td>\n",
       "      <td>Giải trí</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>https://tuoitre.vn/giua-kho-khan-thoi-covid-19...</td>\n",
       "      <td>Giữa khó khăn 'thời COVID-19', một doanh nghiệ...</td>\n",
       "      <td>TTO - Hầu hết doanh nghiệp tại Đồng Nai duy tr...</td>\n",
       "      <td>Ngày 6-1, Sở Lao động - thương binh và xã hội ...</td>\n",
       "      <td>Thời sự</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>https://tuoitre.vn/triet-pha-duong-day-mang-th...</td>\n",
       "      <td>Triệt phá đường dây mang thai hộ, mỗi lần 'đẻ ...</td>\n",
       "      <td>TTO - Công an quận Long Biên (Hà Nội) vừa triệ...</td>\n",
       "      <td>Ngày 6-1, Công an quận Long Biên (Hà Nội) cho ...</td>\n",
       "      <td>Pháp luật</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                links  \\\n",
       "15  https://tuoitre.vn/1-ca-covid-19-moi-phat-hien...   \n",
       "16  https://tuoitre.vn/lang-kinh-24g-canh-giac-voi...   \n",
       "17  https://tuoitre.vn/game-show-cuoi-cung-chi-tai...   \n",
       "18  https://tuoitre.vn/giua-kho-khan-thoi-covid-19...   \n",
       "19  https://tuoitre.vn/triet-pha-duong-day-mang-th...   \n",
       "\n",
       "                                                title  \\\n",
       "15  1 ca COVID-19 mới, phát hiện sau 3 lần xét ngh...   \n",
       "16  Lăng kính 24g: Cảnh giác với tội phạm trộm cắp...   \n",
       "17  Gameshow cuối cùng của Chí Tài - Cơ hội đổi đờ...   \n",
       "18  Giữa khó khăn 'thời COVID-19', một doanh nghiệ...   \n",
       "19  Triệt phá đường dây mang thai hộ, mỗi lần 'đẻ ...   \n",
       "\n",
       "                                          description  \\\n",
       "15  TTO - Sau nhiều ngày số mắc mới khá cao và dồn...   \n",
       "16                                                      \n",
       "17  TTO - Tối 5-1, tập đầu tiên của chương trình t...   \n",
       "18  TTO - Hầu hết doanh nghiệp tại Đồng Nai duy tr...   \n",
       "19  TTO - Công an quận Long Biên (Hà Nội) vừa triệ...   \n",
       "\n",
       "                                              content      class  \n",
       "15  Theo Bộ Y tế, 1 ca mắc mới ngày 6-1 là ca nhập...   Sức khỏe  \n",
       "16                                                      Cần biết  \n",
       "17  Cơ hội đổi đời là chương trình truyền hình thự...   Giải trí  \n",
       "18  Ngày 6-1, Sở Lao động - thương binh và xã hội ...    Thời sự  \n",
       "19  Ngày 6-1, Công an quận Long Biên (Hà Nội) cho ...  Pháp luật  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "for _, row in raw_data.iterrows():\n",
    "    news_page = requests.get(row[\"links\"]).content\n",
    "    news_tree = BeautifulSoup(news_page, \"html.parser\")\n",
    "    # Lấy mô tả\n",
    "    try:\n",
    "        row[\"description\"] = news_tree.find(\"h2\", class_=\"sapo\").text\n",
    "    except:\n",
    "        row[\"description\"] = ''\n",
    "    # Lấy nội dung\n",
    "    try:\n",
    "        body = news_tree.find(\"div\", id=\"main-detail-body\")\n",
    "        content = body.findChildren(\"p\", recursive=False)\n",
    "        row[\"content\"] = \"\"\n",
    "        for x in content:\n",
    "            row[\"content\"] += x.text\n",
    "    except:\n",
    "        row[\"content\"] = ''\n",
    "    time.sleep(0.05)\n",
    "    \n",
    "raw_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vấn đề phát sinh trong quá trình thu thập dữ liệu tự động hàng loạt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thiếu dữ liệu làm đứt quãng quá trình thu thập\n",
    "Do khối lượng dữ liệu có thể thu thập được rất lớn (ước tính lên đến hơn 500.000 tin), việc giám sát quá trình khai thác liên tục là không thể. Việc xuất hiện lỗi này đã khiến quá trình khai thác bị ngắt quãng mỗi khi xuất hiện. Nhóm đã khắc phục bằng cách đặt các khối lệnh ```try``` ```catch``` ở mỗi đoạn code parse các phần thông tin, tự động trả về rỗng nếu có lỗi.\n",
    "\n",
    "#### Lỗi TooManyRedirects: Exceeded 30 redirects\n",
    "Đây là lỗi xuất hiện khi một trang web tự điều hướng quá 30 lần (con số mặc định có thể chỉnh được).\n",
    "\n",
    "Với một số server, khi được request sẽ tùy vào thông tin header của request (như trình duyệt, cache,...), server sẽ đính kèm lệnh chuyển hướng vào response và trình duyệt sẽ dùng thông tin đó điều hướng chuyển qua trang web nhất định. Chẳng hạn, khi người dùng điện thoại truy cập vào trang facebook.com, trình duyệt sẽ tự động điều hướng chuyển người dùng tới trang m.facebook.com là trang có giao diện dành cho điện thoại. Request HTTP trong python cũng không phải ngoại lệ khi có thiết lập mặc định là sẽ tự động chuyển hướng cho đến khi không còn nhận được lệnh từ server.\n",
    "\n",
    "Khi tra cứu lỗi này trên google, nhóm nhận được các đề xuất giải quyết sau:\n",
    "- Thêm tham số ```allow_redirects=True``` vào lệnh ```requests.get```\n",
    "- Thêm tham số ```headers``` vào lệnh ```requests.get```. Tham số này chứa các thông tin mặc định của header mà trình duyệt gửi tới server.\n",
    "- Tạo một ```session``` và thay đổi thuộc tính ```max_redirect``` lên hớn 30.\n",
    "\n",
    "Với cách 1 vô nghĩa do tham số đó là mặc định sẵn, cách 3 thì không đảm bảo thay đổi bao nhiêu là đủ. Còn với cách 2, lệnh request cung cấp thêm thông tin cho server nhằm tránh tình trạng server không xác định được người dùng và rơi vào vòng lặp redirect vô tận. Tuy nhiên nhóm áp dụng và vẫn không khắc phục được. Khi thử lấy link này thì hoàn toàn không vô được.\n",
    "\n",
    "Một điều thú vị là khi debug, nhóm phát hiện rằng các link sinh ra lỗi này đều có nội dung về giáo dục (Hình 4). Khi thử tìm kiếm trang riêng có đường link https://tuyensinh.tuoitre.vn/ thì được điều hướng trở về trang https://tuoitre.vn/giao-duc/tuyen-sinh.htm. Kết luận của nhóm: rất có thể trang báo này đã có quá trình thay đổi công nghệ ở một thời điểm nhất định trong quá khứ khiến cho các trang này không truy cập được -> không chỉ trang tin giáo dục mà còn có thể nhiều trang tin khác cũng bị tương tự.\n",
    "\n",
    "![Hình 4](resources/tuyen-sinh-errors.png \"Hình 4\")\n",
    "\n",
    "Quyết định của nhóm là sẽ áp dụng cách khắc phục 2 vì nó hợp lý và áp khối ```try``` ```catch``` điền thông tin trống với các link tiếp tục bị lỗi.\n",
    "\n",
    "#### Lỗi 502 của trang web được request về\n",
    "Lỗi này đến từ phía server của trang báo và xuất hiện rất ngẫu nhiên không xác định được. Nhóm đã giải quyết bằng cách đặt vòng lặp ```while``` tại các câu lệnh ```get``` và chỉ ngừng khi quá số lần cho phép hoặc nhận phản hồi 200 tức OK. Tuy nhiên cần chú ý cho phép cả phản hồi 301 vì đây là lệnh điều hướng trang có liên quan tới lỗi ở phần trước có đề cập."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thiết lập đoạn code thu thập dữ liệu tự động hàng loạt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def single_request_scraping(index = 1, sleep_time = 0.05):\n",
    "    '''\n",
    "    Thu thập các trang tin trong mục tin mới nhất của báo Tuổi Trẻ.\n",
    "    Mỗi lần thu thập 20 tin. Sử dụng số index để thu thập các trang liên tiếp.\n",
    "    Với index = 1 tức lấy 20 trang tin mới nhất.\n",
    "\n",
    "    Lưu ý:\n",
    "        Khi lấy liên tục cần chọn thời điểm\n",
    "        vì có khả năng báo cập nhật tin tức mới sẽ làm \n",
    "        tin ở trang trước bị đẩy xuống trang sau.\n",
    "\n",
    "    Param:\n",
    "        index: số chỉ trang cần request.\n",
    "    '''\n",
    "    allow_status = [200, 301]\n",
    "    s = requests.Session()\n",
    "    s.headers['User-Agent'] = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/34.0.1847.131 Safari/537.36'\n",
    "    url = f\"https://tuoitre.vn/timeline/0/trang-{index}.htm\"\n",
    "\n",
    "    # Lấy danh sách news_items\n",
    "    try:\n",
    "        response = s.get(url)\n",
    "        try_left = limit_retry\n",
    "        while (response.status_code not in allow_status and try_left > 0):\n",
    "            print(f\"Loi {response.status_code} tai trang {index}\")\n",
    "            response = s.get(url)\n",
    "            try_left -= 1\n",
    "        html_text = response.text\n",
    "    except:\n",
    "        print(f\"Loi request tai trang {index}\")\n",
    "        return None\n",
    "    \n",
    "    html_tree = BeautifulSoup(html_text, 'html.parser')\n",
    "    news_items = html_tree.findAll('li', class_='news-item')\n",
    "    \n",
    "    # Kiểm tra lỗi nếu không lấy được bất cứ item nào\n",
    "    if (len(news_items) == 0):\n",
    "        print(f'Trang {index} khong lay duoc item.')\n",
    "        print(html_text)\n",
    "        return None\n",
    "    elif (len(news_items) != 20):\n",
    "        print(f\"Warning: Trang {index} chi lay duoc {len(news_items)} items.\")\n",
    "    \n",
    "    # Lấy ra link, title và category từ news_items\n",
    "    raw_data = pd.DataFrame(columns=[\"links\",\"title\",\"description\",\"content\",\"class\"])\n",
    "    for item in news_items:\n",
    "        try:\n",
    "            title = item.find('h3', class_='title-news').text.replace('\\n','')\n",
    "        except:\n",
    "            title = \"\"\n",
    "        link = \"https://tuoitre.vn\" + item.find('a').attrs[\"href\"]\n",
    "        try:\n",
    "            category = item.find('a', class_='category-name').text\n",
    "        except:\n",
    "            category = \"\"\n",
    "        raw_data = raw_data.append({\"links\":link, \"title\":title, \"class\":category}, ignore_index=True)\n",
    "    \n",
    "    # Tiến hành lấy nội dung từng link\n",
    "    for _, row in raw_data.iterrows():\n",
    "        try:\n",
    "            response = s.get(row[\"links\"])\n",
    "            try_left = limit_retry\n",
    "            while (response.status_code not in allow_status and try_left > 0):\n",
    "                print(f\"Loi {response.status_code} tai link {row['links']}\")\n",
    "                response = s.get(row[\"links\"])\n",
    "                try_left -= 1\n",
    "            news_page = response.content\n",
    "        except:\n",
    "            print(f\"Loi request tai link {row['links']}\")\n",
    "            row[\"description\"] = \"\"\n",
    "            row[\"content\"] = \"\"\n",
    "            continue\n",
    "            \n",
    "        news_tree = BeautifulSoup(news_page, \"html.parser\")\n",
    "        # Lấy mô tả\n",
    "        try:\n",
    "            row[\"description\"] = news_tree.find(\"h2\", class_=\"sapo\").text\n",
    "        except:\n",
    "            row[\"description\"] = ''\n",
    "        # Lấy nội dung\n",
    "        try:\n",
    "            body = news_tree.find(\"div\", id=\"main-detail-body\")\n",
    "            content = body.findChildren(\"p\", recursive=False)\n",
    "            row[\"content\"] = \"\"\n",
    "            for x in content:\n",
    "                row[\"content\"] += x.text\n",
    "        except:\n",
    "            row[\"content\"] = ''\n",
    "        time.sleep(sleep_time)\n",
    "    \n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 3401 complete!\n",
      "Page 3402 complete!\n",
      "Page 3403 complete!\n",
      "Page 3404 complete!\n",
      "Page 3405 complete!\n",
      "Page 3406 complete!\n",
      "Page 3407 complete!\n",
      "Page 3408 complete!\n",
      "Page 3409 complete!\n",
      "Page 3410 complete!\n",
      "Page 3411 complete!\n",
      "Page 3412 complete!\n",
      "Page 3413 complete!\n",
      "Page 3414 complete!\n",
      "Page 3415 complete!\n",
      "Page 3416 complete!\n",
      "Page 3417 complete!\n",
      "Page 3418 complete!\n",
      "Page 3419 complete!\n",
      "Page 3420 complete!\n",
      "Page 3421 complete!\n",
      "Page 3422 complete!\n",
      "Page 3423 complete!\n",
      "Page 3424 complete!\n",
      "Page 3425 complete!\n",
      "Page 3426 complete!\n",
      "Page 3427 complete!\n",
      "Page 3428 complete!\n",
      "Page 3429 complete!\n",
      "Page 3430 complete!\n",
      "Page 3431 complete!\n",
      "Page 3432 complete!\n",
      "Page 3433 complete!\n",
      "Page 3434 complete!\n",
      "Page 3435 complete!\n",
      "Page 3436 complete!\n",
      "Page 3437 complete!\n",
      "Page 3438 complete!\n",
      "Page 3439 complete!\n",
      "Page 3440 complete!\n",
      "Page 3441 complete!\n",
      "Page 3442 complete!\n",
      "Page 3443 complete!\n",
      "Page 3444 complete!\n",
      "Page 3445 complete!\n",
      "Page 3446 complete!\n",
      "Page 3447 complete!\n",
      "Page 3448 complete!\n",
      "Page 3449 complete!\n",
      "Page 3450 complete!\n",
      "Page 3451 complete!\n",
      "Page 3452 complete!\n",
      "Page 3453 complete!\n",
      "Page 3454 complete!\n",
      "Page 3455 complete!\n",
      "Page 3456 complete!\n",
      "Page 3457 complete!\n",
      "Page 3458 complete!\n",
      "Page 3459 complete!\n",
      "Page 3460 complete!\n",
      "Page 3461 complete!\n",
      "Page 3462 complete!\n",
      "Page 3463 complete!\n",
      "Page 3464 complete!\n",
      "Page 3465 complete!\n",
      "Page 3466 complete!\n",
      "Page 3467 complete!\n",
      "Page 3468 complete!\n",
      "Page 3469 complete!\n",
      "Page 3470 complete!\n",
      "Page 3471 complete!\n",
      "Page 3472 complete!\n",
      "Page 3473 complete!\n",
      "Page 3474 complete!\n",
      "Page 3475 complete!\n",
      "Page 3476 complete!\n",
      "Page 3477 complete!\n",
      "Page 3478 complete!\n",
      "Page 3479 complete!\n",
      "Page 3480 complete!\n",
      "Page 3481 complete!\n",
      "Page 3482 complete!\n",
      "Page 3483 complete!\n",
      "Page 3484 complete!\n",
      "Page 3485 complete!\n",
      "Page 3486 complete!\n",
      "Page 3487 complete!\n",
      "Page 3488 complete!\n",
      "Page 3489 complete!\n",
      "Page 3490 complete!\n",
      "Page 3491 complete!\n",
      "Page 3492 complete!\n",
      "Page 3493 complete!\n",
      "Page 3494 complete!\n",
      "Page 3495 complete!\n",
      "Page 3496 complete!\n",
      "Page 3497 complete!\n",
      "Page 3498 complete!\n",
      "Page 3499 complete!\n",
      "Page 3500 complete!\n",
      "Page 3501 complete!\n",
      "Page 3502 complete!\n",
      "Page 3503 complete!\n",
      "Page 3504 complete!\n",
      "Page 3505 complete!\n",
      "Page 3506 complete!\n",
      "Page 3507 complete!\n",
      "Page 3508 complete!\n",
      "Page 3509 complete!\n",
      "Page 3510 complete!\n",
      "Page 3511 complete!\n",
      "Page 3512 complete!\n",
      "Page 3513 complete!\n",
      "Page 3514 complete!\n",
      "Page 3515 complete!\n",
      "Page 3516 complete!\n",
      "Page 3517 complete!\n",
      "Page 3518 complete!\n",
      "Page 3519 complete!\n",
      "Page 3520 complete!\n",
      "Page 3521 complete!\n",
      "Page 3522 complete!\n",
      "Page 3523 complete!\n",
      "Page 3524 complete!\n",
      "Page 3525 complete!\n",
      "Page 3526 complete!\n",
      "Page 3527 complete!\n",
      "Page 3528 complete!\n",
      "Page 3529 complete!\n",
      "Page 3530 complete!\n",
      "Page 3531 complete!\n",
      "Page 3532 complete!\n",
      "Page 3533 complete!\n",
      "Page 3534 complete!\n",
      "Page 3535 complete!\n",
      "Page 3536 complete!\n",
      "Page 3537 complete!\n",
      "Page 3538 complete!\n",
      "Page 3539 complete!\n",
      "Page 3540 complete!\n",
      "Page 3541 complete!\n",
      "Page 3542 complete!\n",
      "Page 3543 complete!\n",
      "Page 3544 complete!\n",
      "Page 3545 complete!\n",
      "Page 3546 complete!\n",
      "Page 3547 complete!\n",
      "Page 3548 complete!\n",
      "Page 3549 complete!\n",
      "Page 3550 complete!\n",
      "Page 3551 complete!\n",
      "Page 3552 complete!\n",
      "Page 3553 complete!\n",
      "Page 3554 complete!\n",
      "Page 3555 complete!\n",
      "Page 3556 complete!\n",
      "Page 3557 complete!\n",
      "Page 3558 complete!\n",
      "Page 3559 complete!\n",
      "Page 3560 complete!\n",
      "Page 3561 complete!\n",
      "Page 3562 complete!\n",
      "Page 3563 complete!\n",
      "Page 3564 complete!\n",
      "Page 3565 complete!\n",
      "Page 3566 complete!\n",
      "Page 3567 complete!\n",
      "Page 3568 complete!\n",
      "Page 3569 complete!\n",
      "Page 3570 complete!\n",
      "Page 3571 complete!\n",
      "Page 3572 complete!\n",
      "Page 3573 complete!\n",
      "Page 3574 complete!\n",
      "Page 3575 complete!\n",
      "Page 3576 complete!\n",
      "Page 3577 complete!\n",
      "Page 3578 complete!\n",
      "Page 3579 complete!\n",
      "Page 3580 complete!\n",
      "Page 3581 complete!\n",
      "Page 3582 complete!\n",
      "Page 3583 complete!\n",
      "Page 3584 complete!\n",
      "Page 3585 complete!\n",
      "Page 3586 complete!\n",
      "Page 3587 complete!\n",
      "Page 3588 complete!\n",
      "Page 3589 complete!\n",
      "Page 3590 complete!\n",
      "Page 3591 complete!\n",
      "Page 3592 complete!\n",
      "Page 3593 complete!\n",
      "Page 3594 complete!\n",
      "Page 3595 complete!\n",
      "Page 3596 complete!\n",
      "Page 3597 complete!\n",
      "Page 3598 complete!\n",
      "Page 3599 complete!\n",
      "Page 3600 complete!\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'scraped_data\\\\crawling_17.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-38f316603ab7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Page {index} complete!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mbatch_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mbatch_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'scraped_data\\crawling_{iter_num}.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0mcontinue_flag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0miter_num\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors)\u001b[0m\n\u001b[0;32m   3165\u001b[0m             \u001b[0mdecimal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3166\u001b[0m         )\n\u001b[1;32m-> 3167\u001b[1;33m         \u001b[0mformatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3169\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    183\u001b[0m             \u001b[0mclose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m             f, handles = get_handle(\n\u001b[0m\u001b[0;32m    186\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors)\u001b[0m\n\u001b[0;32m    491\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[1;31m# No explicit encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'scraped_data\\\\crawling_17.csv'"
     ]
    }
   ],
   "source": [
    "#Batch scraping\n",
    "iter_num = 1  # Số batch bắt đầu\n",
    "num = 200  # Số trang link trong 1 batch\n",
    "continue_flag = True # Cờ hiệu kết thúc vòng lặp khi xảy ra lỗi\n",
    "\n",
    "while (continue_flag):\n",
    "    '''\n",
    "    Khởi tạo dataframe rỗng. \n",
    "    Sau đó lấy đủ 1 số trang cho 1 batch.\n",
    "    Rồi export file csv.\n",
    "    '''\n",
    "    batch_df = pd.DataFrame(columns=[\"links\",\"title\",\"description\",\"content\",\"class\"])\n",
    "    for index in range(iter_num*num+1,(iter_num+1)*num+1):\n",
    "        data = single_request_scraping(index,0)\n",
    "        if (data is None):\n",
    "            continue_flag = False\n",
    "            break\n",
    "        print(f\"Page {index} complete!\")\n",
    "        batch_df = batch_df.append(data)\n",
    "    batch_df.to_csv(f'scraped_data\\crawling_{iter_num}.csv',index=False,encoding=\"utf-8\")    \n",
    "    iter_num+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Khám phá dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
