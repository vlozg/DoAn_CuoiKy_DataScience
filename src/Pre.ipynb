{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "import re\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0=pd.read_csv('scraped_data/crawling_0.csv')\n",
    "df1=pd.read_csv('scraped_data/crawling_1.csv')\n",
    "df2=pd.read_csv('scraped_data/crawling_2.csv')\n",
    "df3=pd.read_csv('scraped_data/crawling_3.csv')\n",
    "df4=pd.read_csv('scraped_data/crawling_4.csv')\n",
    "df5=pd.read_csv('scraped_data/crawling_5.csv')\n",
    "'''\n",
    "\n",
    "df6=pd.read_csv('scraped_data/crawling_6.csv')\n",
    "df7=pd.read_csv('scraped_data/crawling_7.csv')\n",
    "df8=pd.read_csv('scraped_data/crawling_8.csv')\n",
    "df9=pd.read_csv('scraped_data/crawling_9.csv')\n",
    "df10=pd.read_csv('scraped_data/crawling_10.csv')\n",
    "df11=pd.read_csv('scraped_data/crawling_11.csv')\n",
    "data_df=pd.concat([df0,df1,df2,df3,df4,df5,df6,df7,df8,df9,df10,df11])\n",
    "'''\n",
    "data_df=pd.concat([df0,df1,df2,df3,df4,df5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thời sự',\n",
       " 'Thế giới',\n",
       " 'Thể thao',\n",
       " 'Pháp luật',\n",
       " 'Sức khỏe',\n",
       " 'Kinh doanh',\n",
       " 'Cần biết',\n",
       " 'Giáo dục',\n",
       " 'Nhịp sống trẻ',\n",
       " 'Văn hóa']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_categories = data_df['class'].value_counts()[:10].index.tolist()\n",
    "selected_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>links</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>content</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://tuoitre.vn/tong-thong-trump-xac-nhan-k...</td>\n",
       "      <td>Tổng thống Trump xác nhận không dự lễ nhậm chứ...</td>\n",
       "      <td>TTO - Tổng thống Mỹ Donald Trump đăng tweet ch...</td>\n",
       "      <td>Sau khi đưa ra cam kết sẽ đảm bảo chuyển giao ...</td>\n",
       "      <td>Thế giới</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://tuoitre.vn/dat-nuoc-dat-niem-tin-vao-n...</td>\n",
       "      <td>Đất nước đặt niềm tin vào những học sinh xuất sắc</td>\n",
       "      <td>TT0 - Tối 8-1, Thủ tướng Chính phủ Nguyễn Xuân...</td>\n",
       "      <td>Chia sẻ tại buổi lễ, Thủ tướng Nguyễn Xuân Phú...</td>\n",
       "      <td>Giáo dục</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://tuoitre.vn/luat-su-my-phan-bien-ong-tr...</td>\n",
       "      <td>Luật sư Mỹ phản biện: Ông Trump đâu có kêu ngư...</td>\n",
       "      <td>TTO - Trên chương trình Bill Hemmer Reports củ...</td>\n",
       "      <td>Ngày 6-1, tình trạng bạo lực đã xảy ra tại tòa...</td>\n",
       "      <td>Thế giới</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://tuoitre.vn/thanh-pho-phu-quoc-se-phat-...</td>\n",
       "      <td>Thành phố Phú Quốc sẽ phát triển dựa trên 4 tr...</td>\n",
       "      <td>TTO - Tối 8-1 tại phường An Thới, chính quyền ...</td>\n",
       "      <td>Phát biểu tại buổi lễ công bố thành lập TP Phú...</td>\n",
       "      <td>Thời sự</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://tuoitre.vn/ong-trump-nguoi-ung-ho-toi-...</td>\n",
       "      <td>Ông Trump: Người ủng hộ tôi 'sẽ có tiếng nói t...</td>\n",
       "      <td>TTO - Trong nội dung đăng trên Twitter sau gần...</td>\n",
       "      <td>\"75 triệu người Mỹ yêu nước vĩ đại đã bầu cho ...</td>\n",
       "      <td>Thế giới</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27993</th>\n",
       "      <td>/quy-do-thang-hoa-bang-bang-vao-top-4-co-co-ho...</td>\n",
       "      <td>'Quỷ đỏ' thăng hoa băng băng vào top 4, có cơ ...</td>\n",
       "      <td>TTO - Giành hai danh hiệu, lọt vào top 4 Giải ...</td>\n",
       "      <td>Nửa năm trước, cựu danh thủ người Na Uy còn nằ...</td>\n",
       "      <td>Thể thao</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27995</th>\n",
       "      <td>/bao-chi-can-len-an-hanh-vi-suy-thoai-cua-mot-...</td>\n",
       "      <td>Báo chí cần lên án hành vi suy thoái của một s...</td>\n",
       "      <td>TTO - Đó là đề nghị của Thứ trưởng Bộ Thông ti...</td>\n",
       "      <td>Hội nghị tập huấn bồi dưỡng kiến thức về xây d...</td>\n",
       "      <td>Thời sự</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27996</th>\n",
       "      <td>/kho-hang-lau-khung-tai-lao-cai-thu-tien-ti-tu...</td>\n",
       "      <td>Kho hàng lậu khủng tại Lào Cai: Thu tiền tỉ từ...</td>\n",
       "      <td>TTO - Nhiều sản phẩm thời trang, quần áo, giày...</td>\n",
       "      <td>Đây là vụ kinh doanh hàng giả, hàng lậu online...</td>\n",
       "      <td>Pháp luật</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27997</th>\n",
       "      <td>/ban-giai-phap-go-kho-thuc-kinh-te-tp-hcm-phat...</td>\n",
       "      <td>Bàn giải pháp gỡ khó thúc kinh tế TP.HCM phát ...</td>\n",
       "      <td>TTO - Sáng 9-7, kỳ họp thứ 20 của HĐND TP.HCM ...</td>\n",
       "      <td>Phát biểu khai mạc kỳ họp, sẽ diễn ra trong 3 ...</td>\n",
       "      <td>Thời sự</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27998</th>\n",
       "      <td>/hue-kinh-do-ao-dai-viet-nam-20200709090650965...</td>\n",
       "      <td>Huế - kinh đô áo dài Việt Nam</td>\n",
       "      <td>TTO - “Huế là cái nôi và kinh đô của áo dài Vi...</td>\n",
       "      <td>Đó là lời khẳng định của TS Phan Thanh Hải, gi...</td>\n",
       "      <td>Văn hóa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22302 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   links  \\\n",
       "0      https://tuoitre.vn/tong-thong-trump-xac-nhan-k...   \n",
       "1      https://tuoitre.vn/dat-nuoc-dat-niem-tin-vao-n...   \n",
       "2      https://tuoitre.vn/luat-su-my-phan-bien-ong-tr...   \n",
       "3      https://tuoitre.vn/thanh-pho-phu-quoc-se-phat-...   \n",
       "4      https://tuoitre.vn/ong-trump-nguoi-ung-ho-toi-...   \n",
       "...                                                  ...   \n",
       "27993  /quy-do-thang-hoa-bang-bang-vao-top-4-co-co-ho...   \n",
       "27995  /bao-chi-can-len-an-hanh-vi-suy-thoai-cua-mot-...   \n",
       "27996  /kho-hang-lau-khung-tai-lao-cai-thu-tien-ti-tu...   \n",
       "27997  /ban-giai-phap-go-kho-thuc-kinh-te-tp-hcm-phat...   \n",
       "27998  /hue-kinh-do-ao-dai-viet-nam-20200709090650965...   \n",
       "\n",
       "                                                   title  \\\n",
       "0      Tổng thống Trump xác nhận không dự lễ nhậm chứ...   \n",
       "1      Đất nước đặt niềm tin vào những học sinh xuất sắc   \n",
       "2      Luật sư Mỹ phản biện: Ông Trump đâu có kêu ngư...   \n",
       "3      Thành phố Phú Quốc sẽ phát triển dựa trên 4 tr...   \n",
       "4      Ông Trump: Người ủng hộ tôi 'sẽ có tiếng nói t...   \n",
       "...                                                  ...   \n",
       "27993  'Quỷ đỏ' thăng hoa băng băng vào top 4, có cơ ...   \n",
       "27995  Báo chí cần lên án hành vi suy thoái của một s...   \n",
       "27996  Kho hàng lậu khủng tại Lào Cai: Thu tiền tỉ từ...   \n",
       "27997  Bàn giải pháp gỡ khó thúc kinh tế TP.HCM phát ...   \n",
       "27998                      Huế - kinh đô áo dài Việt Nam   \n",
       "\n",
       "                                             description  \\\n",
       "0      TTO - Tổng thống Mỹ Donald Trump đăng tweet ch...   \n",
       "1      TT0 - Tối 8-1, Thủ tướng Chính phủ Nguyễn Xuân...   \n",
       "2      TTO - Trên chương trình Bill Hemmer Reports củ...   \n",
       "3      TTO - Tối 8-1 tại phường An Thới, chính quyền ...   \n",
       "4      TTO - Trong nội dung đăng trên Twitter sau gần...   \n",
       "...                                                  ...   \n",
       "27993  TTO - Giành hai danh hiệu, lọt vào top 4 Giải ...   \n",
       "27995  TTO - Đó là đề nghị của Thứ trưởng Bộ Thông ti...   \n",
       "27996  TTO - Nhiều sản phẩm thời trang, quần áo, giày...   \n",
       "27997  TTO - Sáng 9-7, kỳ họp thứ 20 của HĐND TP.HCM ...   \n",
       "27998  TTO - “Huế là cái nôi và kinh đô của áo dài Vi...   \n",
       "\n",
       "                                                 content      class  \n",
       "0      Sau khi đưa ra cam kết sẽ đảm bảo chuyển giao ...   Thế giới  \n",
       "1      Chia sẻ tại buổi lễ, Thủ tướng Nguyễn Xuân Phú...   Giáo dục  \n",
       "2      Ngày 6-1, tình trạng bạo lực đã xảy ra tại tòa...   Thế giới  \n",
       "3      Phát biểu tại buổi lễ công bố thành lập TP Phú...    Thời sự  \n",
       "4      \"75 triệu người Mỹ yêu nước vĩ đại đã bầu cho ...   Thế giới  \n",
       "...                                                  ...        ...  \n",
       "27993  Nửa năm trước, cựu danh thủ người Na Uy còn nằ...   Thể thao  \n",
       "27995  Hội nghị tập huấn bồi dưỡng kiến thức về xây d...    Thời sự  \n",
       "27996  Đây là vụ kinh doanh hàng giả, hàng lậu online...  Pháp luật  \n",
       "27997  Phát biểu khai mạc kỳ họp, sẽ diễn ra trong 3 ...    Thời sự  \n",
       "27998  Đó là lời khẳng định của TS Phan Thanh Hải, gi...    Văn hóa  \n",
       "\n",
       "[22302 rows x 5 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = data_df[data_df['class'].isin(selected_categories)]\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df=data_df[~((data_df['title'].isnull())|\n",
    "                  (data_df['description'].isnull())|\n",
    "                  (data_df['content'].isnull()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3997: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "data_df.drop(['links'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "data_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>content</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tổng thống Trump xác nhận không dự lễ nhậm chứ...</td>\n",
       "      <td>TTO - Tổng thống Mỹ Donald Trump đăng tweet ch...</td>\n",
       "      <td>Sau khi đưa ra cam kết sẽ đảm bảo chuyển giao ...</td>\n",
       "      <td>Thế giới</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Đất nước đặt niềm tin vào những học sinh xuất sắc</td>\n",
       "      <td>TT0 - Tối 8-1, Thủ tướng Chính phủ Nguyễn Xuân...</td>\n",
       "      <td>Chia sẻ tại buổi lễ, Thủ tướng Nguyễn Xuân Phú...</td>\n",
       "      <td>Giáo dục</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Luật sư Mỹ phản biện: Ông Trump đâu có kêu ngư...</td>\n",
       "      <td>TTO - Trên chương trình Bill Hemmer Reports củ...</td>\n",
       "      <td>Ngày 6-1, tình trạng bạo lực đã xảy ra tại tòa...</td>\n",
       "      <td>Thế giới</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thành phố Phú Quốc sẽ phát triển dựa trên 4 tr...</td>\n",
       "      <td>TTO - Tối 8-1 tại phường An Thới, chính quyền ...</td>\n",
       "      <td>Phát biểu tại buổi lễ công bố thành lập TP Phú...</td>\n",
       "      <td>Thời sự</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ông Trump: Người ủng hộ tôi 'sẽ có tiếng nói t...</td>\n",
       "      <td>TTO - Trong nội dung đăng trên Twitter sau gần...</td>\n",
       "      <td>\"75 triệu người Mỹ yêu nước vĩ đại đã bầu cho ...</td>\n",
       "      <td>Thế giới</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27993</th>\n",
       "      <td>'Quỷ đỏ' thăng hoa băng băng vào top 4, có cơ ...</td>\n",
       "      <td>TTO - Giành hai danh hiệu, lọt vào top 4 Giải ...</td>\n",
       "      <td>Nửa năm trước, cựu danh thủ người Na Uy còn nằ...</td>\n",
       "      <td>Thể thao</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27995</th>\n",
       "      <td>Báo chí cần lên án hành vi suy thoái của một s...</td>\n",
       "      <td>TTO - Đó là đề nghị của Thứ trưởng Bộ Thông ti...</td>\n",
       "      <td>Hội nghị tập huấn bồi dưỡng kiến thức về xây d...</td>\n",
       "      <td>Thời sự</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27996</th>\n",
       "      <td>Kho hàng lậu khủng tại Lào Cai: Thu tiền tỉ từ...</td>\n",
       "      <td>TTO - Nhiều sản phẩm thời trang, quần áo, giày...</td>\n",
       "      <td>Đây là vụ kinh doanh hàng giả, hàng lậu online...</td>\n",
       "      <td>Pháp luật</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27997</th>\n",
       "      <td>Bàn giải pháp gỡ khó thúc kinh tế TP.HCM phát ...</td>\n",
       "      <td>TTO - Sáng 9-7, kỳ họp thứ 20 của HĐND TP.HCM ...</td>\n",
       "      <td>Phát biểu khai mạc kỳ họp, sẽ diễn ra trong 3 ...</td>\n",
       "      <td>Thời sự</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27998</th>\n",
       "      <td>Huế - kinh đô áo dài Việt Nam</td>\n",
       "      <td>TTO - “Huế là cái nôi và kinh đô của áo dài Vi...</td>\n",
       "      <td>Đó là lời khẳng định của TS Phan Thanh Hải, gi...</td>\n",
       "      <td>Văn hóa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19199 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "0      Tổng thống Trump xác nhận không dự lễ nhậm chứ...   \n",
       "1      Đất nước đặt niềm tin vào những học sinh xuất sắc   \n",
       "2      Luật sư Mỹ phản biện: Ông Trump đâu có kêu ngư...   \n",
       "3      Thành phố Phú Quốc sẽ phát triển dựa trên 4 tr...   \n",
       "4      Ông Trump: Người ủng hộ tôi 'sẽ có tiếng nói t...   \n",
       "...                                                  ...   \n",
       "27993  'Quỷ đỏ' thăng hoa băng băng vào top 4, có cơ ...   \n",
       "27995  Báo chí cần lên án hành vi suy thoái của một s...   \n",
       "27996  Kho hàng lậu khủng tại Lào Cai: Thu tiền tỉ từ...   \n",
       "27997  Bàn giải pháp gỡ khó thúc kinh tế TP.HCM phát ...   \n",
       "27998                      Huế - kinh đô áo dài Việt Nam   \n",
       "\n",
       "                                             description  \\\n",
       "0      TTO - Tổng thống Mỹ Donald Trump đăng tweet ch...   \n",
       "1      TT0 - Tối 8-1, Thủ tướng Chính phủ Nguyễn Xuân...   \n",
       "2      TTO - Trên chương trình Bill Hemmer Reports củ...   \n",
       "3      TTO - Tối 8-1 tại phường An Thới, chính quyền ...   \n",
       "4      TTO - Trong nội dung đăng trên Twitter sau gần...   \n",
       "...                                                  ...   \n",
       "27993  TTO - Giành hai danh hiệu, lọt vào top 4 Giải ...   \n",
       "27995  TTO - Đó là đề nghị của Thứ trưởng Bộ Thông ti...   \n",
       "27996  TTO - Nhiều sản phẩm thời trang, quần áo, giày...   \n",
       "27997  TTO - Sáng 9-7, kỳ họp thứ 20 của HĐND TP.HCM ...   \n",
       "27998  TTO - “Huế là cái nôi và kinh đô của áo dài Vi...   \n",
       "\n",
       "                                                 content      class  \n",
       "0      Sau khi đưa ra cam kết sẽ đảm bảo chuyển giao ...   Thế giới  \n",
       "1      Chia sẻ tại buổi lễ, Thủ tướng Nguyễn Xuân Phú...   Giáo dục  \n",
       "2      Ngày 6-1, tình trạng bạo lực đã xảy ra tại tòa...   Thế giới  \n",
       "3      Phát biểu tại buổi lễ công bố thành lập TP Phú...    Thời sự  \n",
       "4      \"75 triệu người Mỹ yêu nước vĩ đại đã bầu cho ...   Thế giới  \n",
       "...                                                  ...        ...  \n",
       "27993  Nửa năm trước, cựu danh thủ người Na Uy còn nằ...   Thể thao  \n",
       "27995  Hội nghị tập huấn bồi dưỡng kiến thức về xây d...    Thời sự  \n",
       "27996  Đây là vụ kinh doanh hàng giả, hàng lậu online...  Pháp luật  \n",
       "27997  Phát biểu khai mạc kỳ họp, sẽ diễn ra trong 3 ...    Thời sự  \n",
       "27998  Đó là lời khẳng định của TS Phan Thanh Hải, gi...    Văn hóa  \n",
       "\n",
       "[19199 rows x 4 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvi import ViTokenizer, ViPosTagger # thư viện NLP tiếng Việt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import gensim # thư viện NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sau khi đưa ra cam kết sẽ đảm bảo chuyển giao quyền lực một cách hòa bình, êm thắm, ngày 8-1, ông Trump viết trên Twitter: \"Trả lời câu hỏi của các bạn, tôi sẽ không tham dự lễ nhậm chức ngày 20-1\".Dòng tweet này của ông liên tục biến động các con số về lượt người xem và chia sẻ cũng như bình luận. Nhiều người phản đối đã để lại bình luận không mấy thân thiện bên dưới như \"không ai muốn ngài có mặt ở đó\"...Theo Hãng tin Reuters, một nguồn tin am hiểu nội vụ cho biết ở Nhà Trắng đã có sự bàn bạc về việc ông Trump sẽ rời Washington ngày 19-1. Cũng theo nguồn tin này, ông Trump sẽ đến khu nghỉ dưỡng của mình ở bang Florida.Trong lịch sử Mỹ, có nhiều Tổng thống từng quyết định không dự lễ nhậm chức của người kế nhiệm.\\xa0Tổng thống Richard Nixon rời Nhà Trắng khi Gerald Ford tuyên thệ. Ông Nixon từ chức, báo với Phó tổng thống của mình là Gerald Ford vào ngày 8-8 và ông Ford thay mình từ \"ngày hôm nay\".\\xa0 Lúc 9h30 ngày 9-8, ông Nixon có bài phát biểu cuối cùng sau đó rời đi. Khi ông đáp máy bay xuống California, ông Ford đã tuyên thệ xong.\\xa0Ngoài ra, còn có Tổng thống Andrew Johnson năm 1869, John Quincy Adams năm 1829, John Adams năm 1801.'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df[\"content\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "#' '.join(gensim.utils.simple_preprocess(data_df['content'][0]))\n",
    "def gensim_utils_simple_preprocess(x):\n",
    "    return ' '.join(gensim.utils.simple_preprocess(x))\n",
    "data_df['content']=data_df['content'].apply(gensim_utils_simple_preprocess)\n",
    "data_df['class']=data_df['class'].apply(gensim_utils_simple_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:102: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:103: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "def chuan_hoa_dau_tu_tieng_viet(word):\n",
    "    if not is_valid_vietnam_word(word):\n",
    "        return word\n",
    " \n",
    "    chars = list(word)\n",
    "    dau_cau = 0\n",
    "    nguyen_am_index = []\n",
    "    qu_or_gi = False\n",
    "    for index, char in enumerate(chars):\n",
    "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
    "        if x == -1:\n",
    "            continue\n",
    "        elif x == 9:  # check qu\n",
    "            if index != 0 and chars[index - 1] == 'q':\n",
    "                chars[index] = 'u'\n",
    "                qu_or_gi = True\n",
    "        elif x == 5:  # check gi\n",
    "            if index != 0 and chars[index - 1] == 'g':\n",
    "                chars[index] = 'i'\n",
    "                qu_or_gi = True\n",
    "        if y != 0:\n",
    "            dau_cau = y\n",
    "            chars[index] = bang_nguyen_am[x][0]\n",
    "        if not qu_or_gi or index != 1:\n",
    "            nguyen_am_index.append(index)\n",
    "    if len(nguyen_am_index) < 2:\n",
    "        if qu_or_gi:\n",
    "            if len(chars) == 2:\n",
    "                x, y = nguyen_am_to_ids.get(chars[1])\n",
    "                chars[1] = bang_nguyen_am[x][dau_cau]\n",
    "            else:\n",
    "                x, y = nguyen_am_to_ids.get(chars[2], (-1, -1))\n",
    "                if x != -1:\n",
    "                    chars[2] = bang_nguyen_am[x][dau_cau]\n",
    "                else:\n",
    "                    chars[1] = bang_nguyen_am[5][dau_cau] if chars[1] == 'i' else bang_nguyen_am[9][dau_cau]\n",
    "            return ''.join(chars)\n",
    "        return word\n",
    " \n",
    "    for index in nguyen_am_index:\n",
    "        x, y = nguyen_am_to_ids[chars[index]]\n",
    "        if x == 4 or x == 8:  # ê, ơ\n",
    "            chars[index] = bang_nguyen_am[x][dau_cau]\n",
    "            # for index2 in nguyen_am_index:\n",
    "            #     if index2 != index:\n",
    "            #         x, y = nguyen_am_to_ids[chars[index]]\n",
    "            #         chars[index2] = bang_nguyen_am[x][0]\n",
    "            return ''.join(chars)\n",
    " \n",
    "    if len(nguyen_am_index) == 2:\n",
    "        if nguyen_am_index[-1] == len(chars) - 1:\n",
    "            x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
    "            chars[nguyen_am_index[0]] = bang_nguyen_am[x][dau_cau]\n",
    "            # x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
    "            # chars[nguyen_am_index[1]] = bang_nguyen_am[x][0]\n",
    "        else:\n",
    "            # x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
    "            # chars[nguyen_am_index[0]] = bang_nguyen_am[x][0]\n",
    "            x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
    "            chars[nguyen_am_index[1]] = bang_nguyen_am[x][dau_cau]\n",
    "    else:\n",
    "        # x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
    "        # chars[nguyen_am_index[0]] = bang_nguyen_am[x][0]\n",
    "        x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
    "        chars[nguyen_am_index[1]] = bang_nguyen_am[x][dau_cau]\n",
    "        # x, y = nguyen_am_to_ids[chars[nguyen_am_index[2]]]\n",
    "        # chars[nguyen_am_index[2]] = bang_nguyen_am[x][0]\n",
    "    return ''.join(chars)\n",
    "\n",
    "def is_valid_vietnam_word(word):\n",
    "    chars = list(word)\n",
    "    nguyen_am_index = -1\n",
    "    for index, char in enumerate(chars):\n",
    "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
    "        if x != -1:\n",
    "            if nguyen_am_index == -1:\n",
    "                nguyen_am_index = index\n",
    "            else:\n",
    "                if index - nguyen_am_index != 1:\n",
    "                    return False\n",
    "                nguyen_am_index = index\n",
    "    return True\n",
    "\n",
    "def chuan_hoa_dau_cau_tieng_viet(sentence):\n",
    "    \"\"\"\n",
    "        Chuyển câu tiếng việt về chuẩn gõ dấu kiểu cũ.\n",
    "        :param sentence:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "    sentence = sentence.lower()\n",
    "    words = sentence.split()\n",
    "    for index, word in enumerate(words):\n",
    "        cw = re.sub(r'(^\\\\p{P}*)([p{L}.]*\\\\p{L}+)(\\\\p{P}*$)', r'\\\\1/\\\\2/\\\\3', word).split('/')\n",
    "        # print(cw)\n",
    "        if len(cw) == 3:\n",
    "            cw[1] = chuan_hoa_dau_tu_tieng_viet(cw[1])\n",
    "        words[index] = ''.join(cw)\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "\n",
    "data_df['content']=data_df['content'].apply(chuan_hoa_dau_cau_tieng_viet)\n",
    "data_df['class']=data_df['class'].apply(chuan_hoa_dau_cau_tieng_viet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "uniChars = \"àáảãạâầấẩẫậăằắẳẵặèéẻẽẹêềếểễệđìíỉĩịòóỏõọôồốổỗộơờớởỡợùúủũụưừứửữựỳýỷỹỵÀÁẢÃẠÂẦẤẨẪẬĂẰẮẲẴẶÈÉẺẼẸÊỀẾỂỄỆĐÌÍỈĨỊÒÓỎÕỌÔỒỐỔỖỘƠỜỚỞỠỢÙÚỦŨỤƯỪỨỬỮỰỲÝỶỸỴÂĂĐÔƠƯ\"\n",
    "unsignChars = \"aaaaaaaaaaaaaaaaaeeeeeeeeeeediiiiiooooooooooooooooouuuuuuuuuuuyyyyyAAAAAAAAAAAAAAAAAEEEEEEEEEEEDIIIOOOOOOOOOOOOOOOOOOOUUUUUUUUUUUYYYYYAADOOU\"\n",
    "\n",
    "def loaddicchar():\n",
    "    dic = {}\n",
    "    char1252 = 'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ'.split(\n",
    "        '|')\n",
    "    charutf8 = \"à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ\".split(\n",
    "        '|')\n",
    "    for i in range(len(char1252)):\n",
    "        dic[char1252[i]] = charutf8[i]\n",
    "    return dic\n",
    "dicchar = loaddicchar()\n",
    " \n",
    "# Đưa toàn bộ dữ liệu qua hàm này để chuẩn hóa lại\n",
    "def covert_unicode(txt):\n",
    "    return re.sub(\n",
    "        r'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ',\n",
    "        lambda x: dicchar[x.group()], txt)\n",
    "\n",
    "data_df['content']=data_df['content'].apply(covert_unicode)\n",
    "data_df['class']=data_df['class'].apply(covert_unicode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def lower_character(a):\n",
    "    return a.lower()\n",
    "data_df['content']=data_df['content'].apply(lower_character)\n",
    "#xoá ký tự thừa\n",
    "def remove_unnecessary(paragraph):\n",
    "    temp=re.sub(r'[^\\s\\wáàảãạăắằẳẵặâấầẩẫậéèẻẽẹêếềểễệóòỏõọôốồổỗộơớờởỡợíìỉĩịúùủũụưứừửữựýỳỷỹỵđ_]',' ',paragraph)\n",
    "    return re.sub(r'\\s+', ' ', temp).strip()\n",
    "\n",
    "data_df['content']=data_df['content'].apply(remove_unnecessary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "data_df['content']=data_df['content'].apply(ViTokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sau khi tìm hiểu xong ta bắt đầu chia tập dữ liệu và bắt đầu tiền xử lí"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sr = data_df[\"class\"] # sr là viết tắt của series\n",
    "X_df = data_df[\"content\"]\n",
    "train_X_df, val_X_df, train_y_sr, val_y_sr = train_test_split(X_df, y_sr, test_size=0.3,stratify=y_sr, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        sau khi đưa ra cam_kết sẽ đảm_bảo chuyển_giao ...\n",
       "1        chia_sẻ tại buổi lễ thủ_tướng nguyễn xuân phúc...\n",
       "2        ngày tình_trạng bạo_lực đã xảy ra tại tòa nhà ...\n",
       "3        phát_biểu tại buổi lễ công_bố thành_lập tp phú...\n",
       "4        triệu người mỹ yêu nước vĩ_đại đã bầu cho tôi ...\n",
       "                               ...                        \n",
       "27993    nửa năm trước cựu danh_thủ người na uy còn nằm...\n",
       "27995    hội_nghị tập_huấn bồi_dưỡng kiến_thức về xây_d...\n",
       "27996    đây là vụ kinh_doanh hàng giả hàng lậu online ...\n",
       "27997    phát_biểu khai_mạc kỳ họp sẽ diễn ra trong ngà...\n",
       "27998    đó là lời khẳng_định của ts phan thanh hải giá...\n",
       "Name: content, Length: 19199, dtype: object"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chiết xuất stopwords ở từng cell content với TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='\\\\w{1,}', tokenizer=None,\n",
       "                vocabulary=None)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "everything_in_one_big_str =[\" \".join(\" \".join(data_df['content'].tolist()).split(' '))]\n",
    "freq=count_vect.fit_transform(everything_in_one_big_str)\n",
    "count_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37912"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'phát_biểu tại buổi lễ công_bố thành_lập tp phú quốc phó thủ_tướng thường_trực chính_phủ trương hòa bình biểu_dương những nỗ_lực cố_gắng của tỉnh kiên_giang và tp phú quốc thời_gian qua đã đưa phú quốc từ một hòn đảo còn nhiều thiếu_thốn vươn tầm phát_triển mạnh_mẽ theo phó thủ_tướng phú quốc đã có sân_bay cảng biển có điện_lưới quốc_gia có nhiều tập_đoàn lớn đầu_tư nhiều công_trình quy_mô trang_bị hiện_đại đạt tầm khu_vực và thế_giới phú quốc đã có hình_hài một đô_thị thông_minh hiện_đại được nhiều du_khách trong và ngoài nước biết tới cùng với sự phát_triển kinh_tế xã_hội quá_trình phát_triển đô_thị trên địa_bàn phú quốc diễn ra nhanh đã đặt ra yêu_cầu mới cần_thiết phải có một bộ_máy chính_quyền đô_thị có trình_độ phát_triển cao hiện_đại đủ năng_lực thực_thi có hiệu_quả công_tác quản_lý hành_chính nhà_nước trên địa_bàn phó thủ_tướng trương hòa_bình dặn_dò chính_quyền tỉnh kiên giang tp phú quốc tập_trung_thực_hiện một_số nhiệm_vụ trọng_tâm trong đó có việc quy_hoạch phát_triển phú quốc phải có tầm nhìn dài_hạn ít_nhất từ năm trở lên đáp_ứng nhu_cầu phát_triển của cả trước_mắt và lâu_dài tiếp đến là trong phát_triển cơ_sở hạ_tầng đáp_ứng nhu_cầu dịch_vụ cao_cấp nhưng phải giữ_gìn_vẻ đẹp tự_nhiên giữ_gìn cảnh_quan môi_trường và bảo_tồn các giá_trị văn_hóa truyền_thống của phú quốc phát_triển bền_vững bảo_vệ môi_trường biển và hải_đảo nhất là phải bảo_vệ hệ sinh_thái biển sinh_vật biển rạng san_hô xử_lý nước_thải chất_thải chế_tài xử_lý nghiêm đối_với các hành_vi phá_hoại môi_trường trước_mắt cần sớm nghiên_cứu đề_xuất xây_dựng cơ_chế chính_sách đặc_thù đột_phá cho tp phú quốc như mô_hình chính_quyền đô_thị đặc_thù biển đảo cơ_chế chính_sách để thu_hút các nhà đầu_tư lớn có năng_lực thực_sự tạo điều_kiện cho phú quốc phát_triển trở_thành một trung_tâm du_lịch thương_mại lớn của cả nước khu_vực và quốc_tế với trụ_cột chính công_nghiệp giải_trí du_lịch nghỉ_dưỡng dịch_vụ tài_chính ngân_hàng và kinh_tế biển phó thủ_tướng thường_trực chính_phủ cũng gửi_gắm mong_muốn cùng với thành_phố rạch giá hà_tiên phú quốc thành_phố đảo vừa thành_lập sẽ tạo thành trụ_cột để tỉnh kiên_giang phát_triển nhanh bền_vững theo ban quản_lý khu kinh_tế phú quốc trong năm cơ_quan này đã trình ubnd tỉnh chấp_thuận chủ_trương đầu_tư cho dự_án mới với tổng_diện_tích khoảng ha tổng vốn đầu_tư là tỉ đồng tăng dự_án so với năm tính đến nay trên địa_bàn tp phú quốc có dự_án đầu_tư còn hiệu_lực trong các khu quy_hoạch với tổng_diện_tích ha tổng_số vốn đăng_ký là tỉ đồng trong các dự_án đầu_tư nói trên có dự_án phát_triển đô_thị diện_tích khoảng ha ước tổng_vốn tỉ đồng có dự_án phát_triển du_lịch với diện_tích khoảng ha tổng vốn đầu_tư ước khoảng tỉ đồng đến nay có dự_án đầu_tư phú quốc đã đi vào hoạt_động tăng dự_án so với năm trong số này có nhiều dự_án quy_mô cực_kỳ hoành_tráng như quần_thể du_lịch nghỉ_dưỡng có loại_hình dịch_vụ casino của tập_đoàn vingroup phía bắc đảo phú quốc khu nam bắc bãi trường có hàng chục resort sao của các tập_đoàn như ceo bim movenpick novotel phía nam đảo có quần_thể phức_hợp jw mariott kết_hợp_thành chuỗi du_lịch giải_trí đẳng_cấp từ an thới ra hòn thơm với đường cáp treo vượt biển dài nhất thế_giới gần km'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " data_df['content'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 2, 1, ..., 1, 1, 3]], dtype=int64)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "tfidf = TfidfTransformer(norm=\"l2\")\n",
    "tfidf.fit(freq_term_matrix)  \n",
    "\n",
    "feature_names = count_vect.get_feature_names()\n",
    "\n",
    "doc = data_df['content'][3]\n",
    "\n",
    "tf_idf_vector = tfidf.transform(count_vect.transform([doc]))\n",
    "\n",
    "coo_matrix = tf_idf_vector.tocoo()\n",
    "tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "sorted_items = sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    "# extract only the top n elements.\n",
    "# Here, n is 10.\n",
    "word_tfidf = extract_topn_from_vector(feature_names,sorted_items,10)\n",
    "\n",
    "print(\"{}  {}\".format(\"features\", \"tfidf\"))  \n",
    "for k in word_tfidf:\n",
    "    print(\"{} - {}\".format(k[0], k[1])) \n",
    "\n",
    "'''\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=5):\n",
    "    \"\"\"\n",
    "      get the feature names and tf-idf score of top n items in the doc,                 \n",
    "      in descending order of scores. \n",
    "    \"\"\"\n",
    "\n",
    "    # use only top n items from vector.\n",
    "    sorted_items = sorted_items[:topn]\n",
    "\n",
    "    results= {} \n",
    "    # word index and corresponding tf-idf score\n",
    "    for idx, score in sorted_items:\n",
    "        results[feature_names[idx]] = round(score, 3)\n",
    "\n",
    "    # return a sorted list of tuples with feature name and tf-idf score as its element(in descending order of tf-idf scores).\n",
    "    return sorted(results.items(), key=lambda kv: kv[1], reverse=True)\n",
    "\n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "everything_in_one_big_str =[\" \".join(\" \".join(data_df['content'].tolist()).split(' '))]\n",
    "freq_term_matrix = count_vect.fit_transform(everything_in_one_big_str)\n",
    "\n",
    "freq_term_matrix=freq_term_matrix.toarray()\n",
    "extract_topn_from_vector()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(line):\n",
    "    words = []\n",
    "    for word in line.strip().split():\n",
    "        if word not in stopword:\n",
    "            words.append(word)\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a count vectorizer object \n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(train_X_df)\n",
    "\n",
    "# transform the training and validation data using count vectorizer object\n",
    "X_data_count = count_vect.transform(train_X_df)\n",
    "X_test_count = count_vect.transform(val_X_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word level - we choose max number of words equal to 30000 except all words (100k+ words)\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', max_features=30000)\n",
    "tfidf_vect.fit(train_X_df) # learn vocabulary and idf from training set\n",
    "X_data_tfidf =  tfidf_vect.transform(train_X_df)\n",
    "# assume that we don't have test set before\n",
    "X_test_tfidf =  tfidf_vect.transform(val_X_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngram level - we choose max number of words equal to 30000 except all words (100k+ words)\n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', max_features=30000, ngram_range=(2, 3))\n",
    "tfidf_vect_ngram.fit(train_X_df)\n",
    "X_data_tfidf_ngram =  tfidf_vect_ngram.transform(train_X_df)\n",
    "# assume that we don't have test set before\n",
    "X_test_tfidf_ngram =  tfidf_vect_ngram.transform(val_X_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngram-char level - we choose max number of words equal to 30000 except all words (100k+ words)\n",
    "tfidf_vect_ngram_char = TfidfVectorizer(analyzer='char', max_features=30000, ngram_range=(2, 3))\n",
    "tfidf_vect_ngram_char.fit(train_X_df)\n",
    "X_data_tfidf_ngram_char =  tfidf_vect_ngram_char.transform(train_X_df)\n",
    "# assume that we don't have test set before\n",
    "X_test_tfidf_ngram_char =  tfidf_vect_ngram_char.transform(val_X_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=300, random_state=42)\n",
    "svd.fit(X_data_tfidf)\n",
    "\n",
    "\n",
    "X_data_tfidf_svd = svd.transform(X_data_tfidf)\n",
    "X_test_tfidf_svd = svd.transform(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_ngram = TruncatedSVD(n_components=300, random_state=42)\n",
    "svd_ngram.fit(X_data_tfidf_ngram)\n",
    "\n",
    "X_data_tfidf_ngram_svd = svd_ngram.transform(X_data_tfidf_ngram)\n",
    "X_test_tfidf_ngram_svd = svd_ngram.transform(X_test_tfidf_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_ngram_char = TruncatedSVD(n_components=300, random_state=42)\n",
    "svd_ngram_char.fit(X_data_tfidf_ngram_char)\n",
    "\n",
    "X_data_tfidf_ngram_char_svd = svd_ngram_char.transform(X_data_tfidf_ngram_char)\n",
    "X_test_tfidf_ngram_char_svd = svd_ngram_char.transform(X_test_tfidf_ngram_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
